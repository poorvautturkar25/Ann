{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwXLiKmkc6vu+VZ2q0iZrh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poorvautturkar25/Ann/blob/main/23uam135_exp_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6nfW8meVpe0",
        "outputId": "63c19587-a37b-4229-a676-ce8a0ef6f6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Predictions:\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]]\n"
          ]
        }
      ],
      "source": [
        "# Implement Different Loss Functions in a Neural Network\n",
        "import numpy as np\n",
        "\n",
        "# Sample Data (XOR)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "Y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "W = np.random.rand(2, 1)\n",
        "b = np.random.rand(1)\n",
        "lr = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x)) # Indented this line\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x) # Indented this line\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2) # Indented this line\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    eps = 1e-15\n",
        "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) # Indented this line\n",
        "\n",
        "loss_type = 'cross_entropy' # Change to 'mse' to use MSE\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    z = np.dot(X, W) + b\n",
        "    A = sigmoid(z)\n",
        "\n",
        "    if loss_type == 'mse':\n",
        "        loss = mse_loss(Y, A)\n",
        "        d_loss = (Y - A) * sigmoid_derivative(A)\n",
        "    elif loss_type == 'cross_entropy':\n",
        "        loss = cross_entropy_loss(Y, A)\n",
        "        d_loss = (A - Y)\n",
        "\n",
        "    dW = np.dot(X.T, d_loss)\n",
        "    db = np.sum(d_loss)\n",
        "\n",
        "    W -= lr * dW\n",
        "    b -= lr * db\n",
        "\n",
        "\n",
        "output = sigmoid(np.dot(X, W) + b)\n",
        "print(\"\\nFinal Predictions:\")\n",
        "print(output.round())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. House Price Prediction (Regression – MSE Loss)\n",
        "# Problem Statement:\n",
        "# Predict the price of a house based on input features like number of rooms, area (sq ft), and\n",
        "# location index.\n",
        "#  Input: Numerical features (e.g., [3 rooms, 1500 sq ft, location index 0.7])\n",
        "#  Output: Predicted price\n",
        "#  Loss Function: Mean Squared Error (MSE)\n",
        "#  Why MSE? Because output is continuous and exact value prediction matters.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Sample Data (House Price Prediction)\n",
        "X = np.array([[3, 1500, 0.7],\n",
        "              [2, 1000, 0.5],\n",
        "              [4, 2000, 0.9],\n",
        "              [5, 2500, 1.0]])\n",
        "\n",
        "# Normalize input data\n",
        "X_mean = np.mean(X, axis=0)\n",
        "X_std = np.std(X, axis=0)\n",
        "X_normalized = (X - X_mean) / X_std\n",
        "\n",
        "Y = np.array([[300000],\n",
        "              [200000],\n",
        "              [400000],\n",
        "              [500000]])\n",
        "\n",
        "# Initialize weights and bias randomly\n",
        "np.random.seed(0)\n",
        "W = np.random.randn(3, 1) * 0.01\n",
        "b = np.zeros((1, 1))\n",
        "\n",
        "# Learning rate and number of epochs\n",
        "lr = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Define the linear activation function\n",
        "def linear_activation(x):\n",
        "    return x\n",
        "\n",
        "# Define the derivative of the linear activation function\n",
        "def linear_derivative(x):\n",
        "    return np.ones(x.shape)\n",
        "\n",
        "# Define the Mean Squared Error (MSE) loss function\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Define a regression-friendly version of Cross-Entropy loss (not typical usage)\n",
        "def cross_entropy_loss_regression(y_true, y_pred):\n",
        "    eps = 1e-15\n",
        "    y_pred = np.clip(y_pred, eps, np.inf)\n",
        "    return np.mean((y_true - y_pred) ** 2 / (2 * y_pred ** 2) + np.log(y_pred))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    z = np.dot(X_normalized, W) + b\n",
        "    A = linear_activation(z)\n",
        "\n",
        "    # Calculate losses\n",
        "    loss_mse = mse_loss(Y, A)\n",
        "    loss_cross_entropy = cross_entropy_loss_regression(Y, A)\n",
        "\n",
        "    # Backward pass (using MSE for gradient calculation)\n",
        "    d_loss = (A - Y) * linear_derivative(A)\n",
        "\n",
        "    # Calculate gradients\n",
        "    dW = np.dot(X_normalized.T, d_loss)\n",
        "    db = np.sum(d_loss, axis=0, keepdims=True)\n",
        "\n",
        "    # Gradient clipping\n",
        "    dW = np.clip(dW, -1, 1)\n",
        "    db = np.clip(db, -1, 1)\n",
        "\n",
        "    # Update weights and bias\n",
        "    W -= lr * dW\n",
        "    b -= lr * db\n",
        "\n",
        "    # Print losses at certain intervals\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, MSE Loss: {loss_mse}, Cross-Entropy Loss (Regression): {loss_cross_entropy}\")\n",
        "\n",
        "# Make predictions\n",
        "output = linear_activation(np.dot(X_normalized, W) + b)\n",
        "print(\"\\nFinal Predictions:\")\n",
        "print(output)\n",
        "\n",
        "# Example prediction for a new input\n",
        "new_input = np.array([[4, 2200, 0.8]])\n",
        "new_input_normalized = (new_input - X_mean) / X_std\n",
        "predicted_price = linear_activation(np.dot(new_input_normalized, W) + b)\n",
        "print(f\"\\nPredicted price for new input: {predicted_price[0][0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHhsdORrOUwX",
        "outputId": "af46cd02-16db-4956-984c-fccd6cd7755e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, MSE Loss: 134999992994.52713, Cross-Entropy Loss (Regression): 1.6249999999999998e+40\n",
            "Epoch 100, MSE Loss: 134998624469.92982, Cross-Entropy Loss (Regression): 1.6249999999999998e+40\n",
            "Epoch 200, MSE Loss: 134997255965.25075, Cross-Entropy Loss (Regression): 1.6249999999999998e+40\n",
            "Epoch 300, MSE Loss: 134995887480.48988, Cross-Entropy Loss (Regression): 1.6249999999999998e+40\n",
            "Epoch 400, MSE Loss: 134994519015.64723, Cross-Entropy Loss (Regression): 1.6249999999999998e+40\n",
            "Epoch 500, MSE Loss: 134993150570.72281, Cross-Entropy Loss (Regression): 1.6249999999999998e+40\n",
            "Epoch 600, MSE Loss: 134991782145.71663, Cross-Entropy Loss (Regression): 1.6249999999999998e+40\n",
            "Epoch 700, MSE Loss: 134990413740.62866, Cross-Entropy Loss (Regression): 1.6249999999999998e+40\n",
            "Epoch 800, MSE Loss: 134989045355.45892, Cross-Entropy Loss (Regression): 1.6249999999999998e+40\n",
            "Epoch 900, MSE Loss: 134987676990.20743, Cross-Entropy Loss (Regression): 1.6249999999999998e+40\n",
            "\n",
            "Final Predictions:\n",
            "[[ -2.8634405 ]\n",
            " [-31.19664815]\n",
            " [ 25.46976714]\n",
            " [ 48.59032151]]\n",
            "\n",
            "Predicted price for new input: 23.83625427744297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Email Spam Detection (Binary Classification – Cross-Entropy Loss)\n",
        "# Problem Statement:\n",
        "# Classify an email as spam or not based on the frequency of certain keywords, sender address, and\n",
        "# email length.\n",
        "#  Input: Feature vector of email characteristics\n",
        "#  Output: 0 (Not Spam) or 1 (Spam)\n",
        "#  Loss Function: Binary Cross-Entropy Loss\n",
        "#  Why Cross-Entropy? It measures the quality of classification probabilities.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Sample Data\n",
        "X = np.array([[0.5, 0.2, 100],\n",
        "              [0.1, 0.6, 500],\n",
        "              [0.8, 0.4, 200],\n",
        "              [0.3, 0.1, 300]])  # keyword frequency, sender score, email length\n",
        "\n",
        "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)  # normalize X\n",
        "Y = np.array([[1],\n",
        "              [0],\n",
        "              [1],\n",
        "              [0]])  # 1 for spam, 0 for not spam\n",
        "\n",
        "W = np.random.rand(3, 1) * 0.01  # initialize weights with smaller range\n",
        "b = np.random.rand(1) * 0.01\n",
        "lr = 0.01  # decrease learning rate\n",
        "epochs = 1000\n",
        "\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -100, 100)  # clip x to prevent overflow\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    eps = 1e-15\n",
        "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    z = np.dot(X, W) + b\n",
        "    A = sigmoid(z)\n",
        "\n",
        "    loss_mse = mse_loss(Y, A)\n",
        "    loss_cross_entropy = cross_entropy_loss(Y, A)\n",
        "\n",
        "    # Use cross-entropy loss for gradient calculation\n",
        "    d_loss = (A - Y)\n",
        "\n",
        "    dW = np.dot(X.T, d_loss)\n",
        "    db = np.sum(d_loss)\n",
        "\n",
        "    W -= lr * dW\n",
        "    b -= lr * db\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, MSE Loss: {loss_mse:.4f}, Cross-Entropy Loss: {loss_cross_entropy:.4f}\")\n",
        "\n",
        "output = sigmoid(np.dot(X, W) + b)\n",
        "print(\"\\nFinal Predictions:\")\n",
        "print(output.round())\n",
        "\n",
        "new_email = np.array([[0.6, 0.3, 250]])  # keyword frequency, sender score, email length\n",
        "new_email = (new_email - np.mean(X, axis=0)) / np.std(X, axis=0)  # normalize new email\n",
        "predicted_probability = sigmoid(np.dot(new_email, W) + b)\n",
        "print(f\"\\nPredicted probability of being spam: {predicted_probability[0][0]:.4f}\")\n",
        "print(f\"Predicted label: {np.round(predicted_probability)[0][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkuQAhaOO_lI",
        "outputId": "745931a1-5580-4510-fe0f-398242fda908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, MSE Loss: 0.2497, Cross-Entropy Loss: 0.6926\n",
            "Epoch 100, MSE Loss: 0.0457, Cross-Entropy Loss: 0.2207\n",
            "Epoch 200, MSE Loss: 0.0208, Cross-Entropy Loss: 0.1343\n",
            "Epoch 300, MSE Loss: 0.0120, Cross-Entropy Loss: 0.0968\n",
            "Epoch 400, MSE Loss: 0.0078, Cross-Entropy Loss: 0.0758\n",
            "Epoch 500, MSE Loss: 0.0055, Cross-Entropy Loss: 0.0623\n",
            "Epoch 600, MSE Loss: 0.0041, Cross-Entropy Loss: 0.0529\n",
            "Epoch 700, MSE Loss: 0.0032, Cross-Entropy Loss: 0.0460\n",
            "Epoch 800, MSE Loss: 0.0026, Cross-Entropy Loss: 0.0407\n",
            "Epoch 900, MSE Loss: 0.0021, Cross-Entropy Loss: 0.0365\n",
            "\n",
            "Final Predictions:\n",
            "[[1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]]\n",
            "\n",
            "Predicted probability of being spam: 0.0000\n",
            "Predicted label: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Student Placement Prediction (Binary Classification – Cross-Entropy Loss)\n",
        "# Problem Statement:\n",
        "# Predict whether a student will be placed based on academic performance (10th %, 12th %,\n",
        "# CGPA, and IQ score).\n",
        "#  Input: Student feature vector\n",
        "#  Output: 0 (Not Placed) or 1 (Placed)\n",
        "#  Loss Function: Cross-Entropy Loss\n",
        "#  Why Cross-Entropy? The problem is binary classification with probability output.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Sample Data\n",
        "X = np.array([[85, 80, 8.5, 120],  # 10th %, 12th %, CGPA, IQ score\n",
        "              [90, 85, 9.0, 110],\n",
        "              [78, 75, 8.0, 100],\n",
        "              [92, 90, 9.2, 125],\n",
        "              [88, 85, 8.8, 115],\n",
        "              [76, 70, 7.8, 90],\n",
        "              [95, 92, 9.5, 130],\n",
        "              [80, 75, 8.2, 105]])\n",
        "\n",
        "Y = np.array([[1],  # Placed\n",
        "              [1],\n",
        "              [0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "# Initialize weights and bias\n",
        "W = np.random.rand(4, 1)\n",
        "b = np.random.rand(1)\n",
        "\n",
        "# Learning rate and epochs\n",
        "lr = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "# Stable sigmoid function\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -100, 100)  # Clip x to prevent overflow\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Sigmoid derivative\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Cross-entropy loss function\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    eps = 1e-15\n",
        "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "# Loss type\n",
        "loss_type = 'cross_entropy'\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    z = np.dot(X, W) + b\n",
        "    A = sigmoid(z)\n",
        "\n",
        "    if loss_type == 'cross_entropy':\n",
        "        loss = cross_entropy_loss(Y, A)\n",
        "        d_loss = (A - Y)\n",
        "\n",
        "    dW = np.dot(X.T, d_loss)\n",
        "    db = np.sum(d_loss)\n",
        "\n",
        "    W -= lr * dW\n",
        "    b -= lr * db\n",
        "\n",
        "output = sigmoid(np.dot(X, W) + b)\n",
        "print(\"\\nFinal prediction:\")\n",
        "print(output.round())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il_f8YVYPjb_",
        "outputId": "d7d3c35c-9982-4184-c7c7-1303acb63cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final prediction:\n",
            "[[1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Temperature Forecasting (Regression – MSE Loss)\n",
        "# Problem Statement:\n",
        "#Forecast next-day temperature using historical weather data.\n",
        "#  Input: Time series features of temperature, humidity, wind speed, etc.\n",
        "#  Output: Forecasted temperature\n",
        "#  Loss Function: Mean Squared Error\n",
        "#  Why MSE? Because prediction accuracy of real values is important.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Sample Data (Temperature Forecasting)\n",
        "X = np.array([[20, 60, 5],\n",
        "              [22, 50, 10],\n",
        "              [18, 70, 3],\n",
        "              [25, 40, 15]])\n",
        "\n",
        "# Corresponding temperature values for the next day\n",
        "Y = np.array([[22],\n",
        "              [24],\n",
        "              [19],\n",
        "              [26]])\n",
        "\n",
        "# Feature scaling\n",
        "X_mean = np.mean(X, axis=0)\n",
        "X_std = np.std(X, axis=0)\n",
        "X = (X - X_mean) / X_std\n",
        "\n",
        "# Initialize weights and bias randomly\n",
        "W = np.random.rand(3, 1)\n",
        "b = np.random.rand(1)\n",
        "\n",
        "# Learning rate and number of epochs\n",
        "lr = 0.0001  # Significantly reduced learning rate\n",
        "epochs = 1000\n",
        "\n",
        "# Define the linear activation function\n",
        "def linear_activation(x):\n",
        "    return x\n",
        "\n",
        "# Define the derivative of the linear activation function\n",
        "def linear_derivative(x):\n",
        "    return np.ones(x.shape)\n",
        "\n",
        "# Define the Mean Squared Error (MSE) loss function\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Define a regression-friendly version of Cross-Entropy loss (not typical usage)\n",
        "def cross_entropy_loss_regression(y_true, y_pred):\n",
        "    eps = 1e-15\n",
        "    y_pred = np.clip(y_pred, eps, np.inf)\n",
        "    return np.mean((y_true - y_pred) ** 2 / (2 * y_pred ** 2) + np.log(y_pred))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    z = np.dot(X, W) + b\n",
        "    A = linear_activation(z)\n",
        "\n",
        "    # Calculate losses\n",
        "    loss_mse = mse_loss(Y, A)\n",
        "    loss_cross_entropy = cross_entropy_loss_regression(Y, np.abs(A))  # Use absolute values for cross-entropy\n",
        "\n",
        "    # Backward pass (using MSE for gradient calculation)\n",
        "    d_loss = (A - Y) * linear_derivative(A)\n",
        "\n",
        "    # Calculate gradients\n",
        "    dW = np.dot(X.T, d_loss)\n",
        "    db = np.sum(d_loss)\n",
        "\n",
        "    # Update weights and bias\n",
        "    W -= lr * dW\n",
        "    b -= lr * db\n",
        "\n",
        "    # Print losses at certain intervals\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, MSE Loss: {loss_mse}, Cross-Entropy Loss (Regression): {loss_cross_entropy}\")\n",
        "\n",
        "# Make predictions\n",
        "output = linear_activation(np.dot(X, W) + b)\n",
        "print(\"\\nFinal Predictions:\")\n",
        "print(output)\n",
        "\n",
        "# Example prediction for a new input\n",
        "new_input = np.array([[23, 55, 8]])  # temperature, humidity, wind speed\n",
        "new_input = (new_input - X_mean) / X_std  # Scale new input\n",
        "predicted_temperature = linear_activation(np.dot(new_input, W) + b)\n",
        "print(f\"\\nPredicted temperature for new input: {predicted_temperature[0][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIcBk8EVQG4W",
        "outputId": "7d7910a6-ebd3-4e86-9fd2-1872bfad1513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, MSE Loss: 514.5789081174728, Cross-Entropy Loss (Regression): 572.097206788924\n",
            "Epoch 100, MSE Loss: 474.7610501840255, Cross-Entropy Loss (Regression): 1814.749090918225\n",
            "Epoch 200, MSE Loss: 438.06333793353343, Cross-Entropy Loss (Regression): 1019.5373962832267\n",
            "Epoch 300, MSE Loss: 404.23347564910665, Cross-Entropy Loss (Regression): 79.1310595705188\n",
            "Epoch 400, MSE Loss: 373.04119213016924, Cross-Entropy Loss (Regression): 30.107834703310946\n",
            "Epoch 500, MSE Loss: 344.2759903047946, Cross-Entropy Loss (Regression): 16.463024089339292\n",
            "Epoch 600, MSE Loss: 317.7451880450763, Cross-Entropy Loss (Regression): 10.72764823071813\n",
            "Epoch 700, MSE Loss: 293.27220271808915, Cross-Entropy Loss (Regression): 7.797699736505824\n",
            "Epoch 800, MSE Loss: 270.6950409765037, Cross-Entropy Loss (Regression): 6.119290373155934\n",
            "Epoch 900, MSE Loss: 249.8649624452274, Cross-Entropy Loss (Regression): 5.0828278910779945\n",
            "\n",
            "Final Predictions:\n",
            "[[ 6.32114849]\n",
            " [ 8.27004131]\n",
            " [ 5.07490711]\n",
            " [10.65676144]]\n",
            "\n",
            "Predicted temperature for new input: 8.288358093016228\n"
          ]
        }
      ]
    }
  ]
}
