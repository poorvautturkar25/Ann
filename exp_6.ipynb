{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTaImXliEc1BcKoR8U8i46",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poorvautturkar25/Ann/blob/main/23uam135_exp_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoOmfp6Xjkc3",
        "outputId": "a53c5a6a-c3c4-4d82-9362-da8a49b91bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1, Loss: 0.16528508994211122\n",
            "Epochs: 2, Loss: 0.16480734430724608\n",
            "Epochs: 3, Loss: 0.16433310707340976\n",
            "Epochs: 4, Loss: 0.16386233873178968\n",
            "Epochs: 5, Loss: 0.1633950003586766\n",
            "Epochs: 6, Loss: 0.16293105360492943\n",
            "Epochs: 7, Loss: 0.16247046068566084\n",
            "Epochs: 8, Loss: 0.16201318437013523\n",
            "Epochs: 9, Loss: 0.16155918797187796\n",
            "Epochs: 10, Loss: 0.1611084353389871\n",
            "Epochs: 11, Loss: 0.16066089084464552\n",
            "Epochs: 12, Loss: 0.16021651937782788\n",
            "Epochs: 13, Loss: 0.15977528633419902\n",
            "Epochs: 14, Loss: 0.15933715760719636\n",
            "Epochs: 15, Loss: 0.1589020995792968\n",
            "Epochs: 16, Loss: 0.15847007911346056\n",
            "Epochs: 17, Loss: 0.15804106354474723\n",
            "Epochs: 18, Loss: 0.15761502067210453\n",
            "Epochs: 19, Loss: 0.15719191875032068\n",
            "Epochs: 20, Loss: 0.15677172648213944\n",
            "Epochs: 21, Loss: 0.15635441301053588\n",
            "Epochs: 22, Loss: 0.15593994791114296\n",
            "Epochs: 23, Loss: 0.15552830118483418\n",
            "Epochs: 24, Loss: 0.15511944325045102\n",
            "Epochs: 25, Loss: 0.15471334493767686\n",
            "Epochs: 26, Loss: 0.1543099774800516\n",
            "Epochs: 27, Loss: 0.15390931250812556\n",
            "Epochs: 28, Loss: 0.15351132204274764\n",
            "Epochs: 29, Loss: 0.15311597848848713\n",
            "Epochs: 30, Loss: 0.15272325462718317\n",
            "Epochs: 31, Loss: 0.1523331236116222\n",
            "Epochs: 32, Loss: 0.15194555895933892\n",
            "Epochs: 33, Loss: 0.15156053454653706\n",
            "Epochs: 34, Loss: 0.15117802460213048\n",
            "Epochs: 35, Loss: 0.15079800370189755\n",
            "Epochs: 36, Loss: 0.15042044676275135\n",
            "Epochs: 37, Loss: 0.1500453290371182\n",
            "Epochs: 38, Loss: 0.14967262610742682\n",
            "Epochs: 39, Loss: 0.14930231388070103\n",
            "Epochs: 40, Loss: 0.14893436858325815\n",
            "Epochs: 41, Loss: 0.1485687667555078\n",
            "Epochs: 42, Loss: 0.14820548524684984\n",
            "Epochs: 43, Loss: 0.14784450121066972\n",
            "Epochs: 44, Loss: 0.14748579209942903\n",
            "Epochs: 45, Loss: 0.14712933565984831\n",
            "Epochs: 46, Loss: 0.14677510992818177\n",
            "Epochs: 47, Loss: 0.14642309322557967\n",
            "Epochs: 48, Loss: 0.14607326415353927\n",
            "Epochs: 49, Loss: 0.14572560158944015\n",
            "Epochs: 50, Loss: 0.1453800846821629\n",
            "Epochs: 51, Loss: 0.1450366928477903\n",
            "Epochs: 52, Loss: 0.14469540576538753\n",
            "Epochs: 53, Loss: 0.14435620337286126\n",
            "Epochs: 54, Loss: 0.14401906586289492\n",
            "Epochs: 55, Loss: 0.14368397367895913\n",
            "Epochs: 56, Loss: 0.14335090751139623\n",
            "Epochs: 57, Loss: 0.14301984829357484\n",
            "Epochs: 58, Loss: 0.14269077719811762\n",
            "Epochs: 59, Loss: 0.14236367563319485\n",
            "Epochs: 60, Loss: 0.1420385252388885\n",
            "Epochs: 61, Loss: 0.14171530788361963\n",
            "Epochs: 62, Loss: 0.14139400566064286\n",
            "Epochs: 63, Loss: 0.14107460088460233\n",
            "Epochs: 64, Loss: 0.1407570760881519\n",
            "Epochs: 65, Loss: 0.1404414140186333\n",
            "Epochs: 66, Loss: 0.14012759763481708\n",
            "Epochs: 67, Loss: 0.1398156101036988\n",
            "Epochs: 68, Loss: 0.139505434797355\n",
            "Epochs: 69, Loss: 0.1391970552898535\n",
            "Epochs: 70, Loss: 0.13889045535421884\n",
            "Epochs: 71, Loss: 0.1385856189594531\n",
            "Epochs: 72, Loss: 0.13828253026760617\n",
            "Epochs: 73, Loss: 0.13798117363090057\n",
            "Epochs: 74, Loss: 0.13768153358890578\n",
            "Epochs: 75, Loss: 0.13738359486576113\n",
            "Epochs: 76, Loss: 0.13708734236745002\n",
            "Epochs: 77, Loss: 0.13679276117911754\n",
            "Epochs: 78, Loss: 0.13649983656243936\n",
            "Epochs: 79, Loss: 0.13620855395303233\n",
            "Epochs: 80, Loss: 0.13591889895791287\n",
            "Epochs: 81, Loss: 0.13563085735299685\n",
            "Epochs: 82, Loss: 0.1353444150806451\n",
            "Epochs: 83, Loss: 0.13505955824724836\n",
            "Epochs: 84, Loss: 0.13477627312085594\n",
            "Epochs: 85, Loss: 0.13449454612884393\n",
            "Epochs: 86, Loss: 0.13421436385562213\n",
            "Epochs: 87, Loss: 0.13393571304038232\n",
            "Epochs: 88, Loss: 0.13365858057488322\n",
            "Epochs: 89, Loss: 0.13338295350127294\n",
            "Epochs: 90, Loss: 0.13310881900994842\n",
            "Epochs: 91, Loss: 0.1328361644374514\n",
            "Epochs: 92, Loss: 0.1325649772643992\n",
            "Epochs: 93, Loss: 0.1322952451134498\n",
            "Epochs: 94, Loss: 0.13202695574730228\n",
            "Epochs: 95, Loss: 0.13176009706672942\n",
            "Epochs: 96, Loss: 0.13149465710864305\n",
            "Epochs: 97, Loss: 0.13123062404419228\n",
            "Epochs: 98, Loss: 0.13096798617689254\n",
            "Epochs: 99, Loss: 0.13070673194078608\n",
            "Epochs: 100, Loss: 0.13044684989863176\n"
          ]
        }
      ],
      "source": [
        "# Implement Backpropagation in a Simple MLP\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid_function(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def derivative(x):\n",
        "  return x*(1-x)\n",
        "\n",
        "#take inputs\n",
        "x1 = 0.4\n",
        "x2 = 0.8\n",
        "\n",
        "#actual output\n",
        "y_actual = 1\n",
        "\n",
        "#take weight and biases\n",
        "w1,w2,w3,w4 = np.random.rand(4)\n",
        "b1,b2 = np.random.rand(2)\n",
        "\n",
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "losses = []\n",
        "iterations =0\n",
        "\n",
        "for iterations in range(epochs):\n",
        "  #forward propagation\n",
        "  # for hidden layer\n",
        "  z_hid = w1*x1 + w2*x2 + b1\n",
        "  h = sigmoid_function(z_hid)\n",
        "\n",
        "  #for output layer\n",
        "  z_out = w3*h + w4*h + b2\n",
        "  y_pred = sigmoid_function(z_out)\n",
        "\n",
        "  #error calculation\n",
        "  # The original line was trying to call a float as a function.\n",
        "  # Corrected line to perform multiplication:\n",
        "  E = (1/2)*(y_actual - y_pred)*2\n",
        "  losses.append(E)\n",
        "\n",
        "  #Backpropagation\n",
        "  dE_dypred = -(y_actual - y_pred)\n",
        "  dypred_dh = derivative(y_pred)*(w3 + w4)\n",
        "  dh_dw1 = derivative(h)*x1\n",
        "  dh_dw2 = derivative(h)*x2\n",
        "\n",
        "  #gradients calculation\n",
        "  dw3 = dE_dypred * derivative(y_pred)*h\n",
        "  dw4 = dE_dypred * derivative(y_pred)*h\n",
        "  dw1 = dE_dypred * dypred_dh * dh_dw1\n",
        "  dw2 = dE_dypred * dypred_dh * dh_dw2\n",
        "  # db1 = dE_dypred * dypred_dh * derivative(h)\n",
        "  # db2 = dE_dypred * derivative(y_pred)\n",
        "\n",
        "  #update weight and bias\n",
        "  w1 = w1 - learning_rate*dw1\n",
        "  w2 = w2 - learning_rate*dw2\n",
        "  w3 = w3 - learning_rate*dw3\n",
        "  w4 = w4 - learning_rate*dw4\n",
        "  d1 = b1 - learning_rate*dw1\n",
        "  d2 = b2 - learning_rate*dw2\n",
        "\n",
        "\n",
        "\n",
        "  if iterations % 100 == iterations:\n",
        "    iterations = iterations +1\n",
        "    print(f\"Epochs: {iterations}, Loss: {E}\")\n",
        "  iterations += 1"
      ]
    }
  ]
}
